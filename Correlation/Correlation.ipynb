{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c409563f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We import essential libraries for **data handling, visualization, and statistical analysis**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```\n",
    "\n",
    "### **Breaking Down Each Import:**\n",
    "1. **NumPy (`numpy`)**: \n",
    "   - Used for numerical computations.\n",
    "   - Helps in handling arrays and performing mathematical operations.\n",
    "\n",
    "2. **Pandas (`pandas`)**: \n",
    "   - A powerful library for handling structured data.\n",
    "   - Used for reading datasets and performing correlation computations.\n",
    "\n",
    "3. **Matplotlib (`matplotlib.pyplot`)**:\n",
    "   - Used for data visualization.\n",
    "   - Provides basic plotting functions.\n",
    "\n",
    "4. **Seaborn (`seaborn`)**:\n",
    "   - Built on Matplotlib but provides **better statistical visualizations**.\n",
    "   - Used for scatter plots, heatmaps, and pair plots in correlation analysis.\n",
    "\n",
    "5. **Warnings (`warnings.filterwarnings('ignore')`)**:\n",
    "   - Suppresses unnecessary warnings for cleaner outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd865d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b64b2",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Load the Dataset\n",
    "\n",
    "We load the **Palmer Penguins dataset** using **Seaborn’s built-in dataset loader**.\n",
    "\n",
    "```python\n",
    "penguin = sns.load_dataset('penguins')\n",
    "penguin.head()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "1. **`sns.load_dataset('penguins')`**:\n",
    "   - Loads the `penguins` dataset from Seaborn’s built-in collection.\n",
    "   - Contains measurements of **different penguin species** in Antarctica.\n",
    "\n",
    "2. **`penguin.head()`**:\n",
    "   - Displays the **first five rows** of the dataset.\n",
    "   - Helps us quickly inspect the structure.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- We need to understand the **features** available before performing correlation analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin = sns.load_dataset('penguins')\n",
    "penguin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad31961",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before analyzing correlations, we **explore** the dataset:\n",
    "\n",
    "```python\n",
    "penguin.describe()\n",
    "penguin.info()\n",
    "penguin.isnull().sum()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`penguin.describe()`**:\n",
    "   - Generates **summary statistics** for numerical columns (mean, std, min, max, etc.).\n",
    "\n",
    "2. **`penguin.info()`**:\n",
    "   - Shows **data types**, missing values, and column counts.\n",
    "\n",
    "3. **`penguin.isnull().sum()`**:\n",
    "   - Counts missing values in each column.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- Helps identify **data issues** before correlation analysis.\n",
    "- If many missing values exist, correlation results might be **biased**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196da99c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Handling Missing Values\n",
    "\n",
    "Missing values can distort correlation computations. We need to decide whether to:\n",
    "\n",
    "1. **Drop rows with missing values** (if there aren’t too many missing values).\n",
    "2. **Impute missing values** using the mean/median/mode.\n",
    "3. **Use pairwise correlation** (automatically handles missing values).\n",
    "\n",
    "### **Steps We Take:**\n",
    "\n",
    "```python\n",
    "penguin.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "- **`dropna(inplace=True)`**:  \n",
    "  - Removes all rows containing missing values.\n",
    "  - `inplace=True` ensures changes are applied directly.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- If we **ignore missing values**, correlation results might be **misleading**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y = 'bill_length_mm', x = 'body_mass_g', data=penguin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938da543",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Visualizing Correlations using Scatterplots\n",
    "\n",
    "A **scatterplot** helps visualize **relationships between variables**.\n",
    "\n",
    "```python\n",
    "sns.scatterplot(y='bill_length_mm', x='body_mass_g', data=penguin)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "1. **`sns.scatterplot(y='bill_length_mm', x='body_mass_g', data=penguin)`**:\n",
    "   - Plots a **scatterplot** with:\n",
    "     - **X-axis (`body_mass_g`)**: Penguin body mass.\n",
    "     - **Y-axis (`bill_length_mm`)**: Penguin bill length.\n",
    "\n",
    "2. **`plt.show()`**:\n",
    "   - Displays the scatter plot.\n",
    "\n",
    "### **How to Interpret?**\n",
    "- A **positive trend** suggests a **positive correlation**.\n",
    "- A **downward trend** suggests a **negative correlation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin.corr(numeric_only= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8ff2f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Computing Pearson Correlation\n",
    "\n",
    "We compute **Pearson’s correlation coefficient**:\n",
    "\n",
    "```python\n",
    "penguin.corr(numeric_only=True)\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "1. **`penguin.corr(numeric_only=True)`**:\n",
    "   - Computes Pearson’s correlation for **numeric columns only**.\n",
    "   - Pearson’s correlation measures **linear relationships** between variables.\n",
    "\n",
    "### **How to Interpret Pearson Correlation?**\n",
    "- **+1.0** → Perfect **positive** correlation.\n",
    "- **-1.0** → Perfect **negative** correlation.\n",
    "- **0.0** → No correlation.\n",
    "\n",
    "⚠️ **Pearson correlation assumes linear relationships.**  \n",
    "If relationships are **non-linear**, Spearman/Kendall correlation should be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = penguin.corr(numeric_only= True)\n",
    "upper = np.triu(val)\n",
    "lower = np.tril(val)\n",
    "sns.heatmap(val, cmap = 'RdYlGn', annot = True, mask = lower)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65651888",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7: Correlation Heatmap\n",
    "\n",
    "A **heatmap** provides a **visual representation** of correlations.\n",
    "\n",
    "```python\n",
    "val = penguin.corr(numeric_only=True)\n",
    "sns.heatmap(val, cmap='RdYlGn', annot=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "1. **`penguin.corr(numeric_only=True)`** → Computes Pearson correlation matrix.\n",
    "2. **`sns.heatmap(val, cmap='RdYlGn', annot=True)`**:\n",
    "   - Plots a **heatmap**.\n",
    "   - **`cmap='RdYlGn'`** → Uses Red-Yellow-Green color scheme.\n",
    "   - **`annot=True`** → Displays correlation values on the heatmap.\n",
    "\n",
    "### **How to Interpret the Heatmap?**\n",
    "- **Dark Green** → Strong **positive** correlation.\n",
    "- **Dark Red** → Strong **negative** correlation.\n",
    "- **Yellow** → Weak or no correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(penguin,\n",
    "             y_vars = ['body_mass_g'],\n",
    "             x_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], height = 5, aspect = 0.8, kind = 'kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918e755",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8: Additional Correlation Methods (Spearman & Kendall)\n",
    "\n",
    "Pearson correlation works **only for linear relationships**.  \n",
    "We also check **Spearman** and **Kendall** correlation, which handle **non-linear relationships**.\n",
    "\n",
    "```python\n",
    "penguin.corr(method='spearman', numeric_only=True)\n",
    "penguin.corr(method='kendall', numeric_only=True)\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**  \n",
    "1. **`penguin.corr(method='spearman', numeric_only=True)`**:  \n",
    "   - Computes **Spearman Rank Correlation**.  \n",
    "   - Measures **monotonic** relationships (order-based, but not necessarily linear).  \n",
    "\n",
    "2. **`penguin.corr(method='kendall', numeric_only=True)`**:  \n",
    "   - Computes **Kendall Tau Correlation**.  \n",
    "   - More robust for small datasets or data with **many tied ranks**.\n",
    "\n",
    "### **Why Use Spearman & Kendall?**  \n",
    "- If data has **non-linear** relationships, **Pearson fails** to capture correlations.  \n",
    "- These methods work better for **ordinal** or **rank-based** data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(penguin,\n",
    "             y_vars = ['body_mass_g'],\n",
    "             x_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'],\n",
    "             height = 5,\n",
    "             aspect = 0.8,\n",
    "             kind = 'reg',\n",
    "             hue = 'species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56f6fd",
   "metadata": {},
   "source": [
    "\n",
    "## Step 9: Hypothesis Testing for Correlation\n",
    "\n",
    "Just because two variables have **a correlation value**, we need to test if it's **statistically significant**.  \n",
    "\n",
    "We use **p-values** to check if the correlation is real.  \n",
    "\n",
    "```python\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "pearson_corr, pearson_p = pearsonr(penguin['bill_length_mm'], penguin['body_mass_g'])\n",
    "spearman_corr, spearman_p = spearmanr(penguin['bill_length_mm'], penguin['body_mass_g'])\n",
    "\n",
    "print(f\"Pearson Correlation: {pearson_corr}, p-value: {pearson_p}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr}, p-value: {spearman_p}\")\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**  \n",
    "1. **`pearsonr(penguin['bill_length_mm'], penguin['body_mass_g'])`**:  \n",
    "   - Computes Pearson’s **correlation coefficient** and **p-value**.  \n",
    "   - If **p-value < 0.05**, correlation is **statistically significant**.  \n",
    "\n",
    "2. **`spearmanr(penguin['bill_length_mm'], penguin['body_mass_g'])`**:  \n",
    "   - Computes Spearman’s **rank correlation** and **p-value**.  \n",
    "   - If **p-value < 0.05**, we reject H₀ (**correlation is significant**).  \n",
    "\n",
    "### **How to Interpret?**  \n",
    "- **p < 0.05** → Significant correlation.  \n",
    "- **p > 0.05** → No strong evidence of correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in penguin:\n",
    "  if penguin[cols].dtype == 'O':\n",
    "    print(cols, ':', penguin[cols].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be13e5f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 10: Checking Multicollinearity with Variance Inflation Factor (VIF)\n",
    "\n",
    "If two or more variables are **highly correlated**, they cause **multicollinearity**,  \n",
    "which **reduces model reliability** in regression analysis.  \n",
    "\n",
    "We use **Variance Inflation Factor (VIF)** to detect multicollinearity.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Select numeric columns\n",
    "num_cols = penguin.select_dtypes(include=['float64', 'int64']).dropna()\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = num_cols.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(num_cols.values, i) for i in range(len(num_cols.columns))]\n",
    "\n",
    "print(vif_data)\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**  \n",
    "1. **`variance_inflation_factor(num_cols.values, i)`**:  \n",
    "   - Computes **VIF score** for feature `i`.  \n",
    "   - VIF > **5** suggests **multicollinearity** is a concern.  \n",
    "\n",
    "2. **`num_cols.select_dtypes(include=['float64', 'int64']).dropna()`**:  \n",
    "   - Ensures only **numeric columns** are analyzed (ignoring categorical).  \n",
    "\n",
    "### **How to Interpret VIF Results?**  \n",
    "- **VIF < 5** → No multicollinearity concern.  \n",
    "- **VIF > 5** → Moderate multicollinearity.  \n",
    "- **VIF > 10** → Severe multicollinearity; remove one of the correlated variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a014a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " sns.jointplot(penguin,\n",
    "             x = 'bill_length_mm',\n",
    "             y = 'body_mass_g',\n",
    "             kind = 'kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32918c2a",
   "metadata": {},
   "source": [
    "\n",
    "## Step 11: Outlier Detection & Its Effect on Correlation\n",
    "\n",
    "Outliers can **inflate or distort correlation values**.  \n",
    "We detect outliers using **boxplots**.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=penguin[['bill_length_mm', 'body_mass_g']], palette='Set2')\n",
    "plt.title('Boxplot of Variables')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**  \n",
    "1. **`sns.boxplot(data=penguin[['bill_length_mm', 'body_mass_g']], palette='Set2')`**:  \n",
    "   - Plots a **boxplot** to detect **outliers** in numerical variables.  \n",
    "\n",
    "2. **`plt.show()`**:  \n",
    "   - Displays the plot.\n",
    "\n",
    "### **How to Handle Outliers?**  \n",
    "- **Winsorization**: Replacing extreme values with percentiles.  \n",
    "- **Removing Outliers**: If they distort correlation.  \n",
    "- **Transforming Data**: Using log/square root transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd5cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c886a64",
   "metadata": {},
   "source": [
    "\n",
    "## Step 12: Computing Correlation Before & After Removing Outliers\n",
    "\n",
    "We check how removing outliers **changes correlation values**.\n",
    "\n",
    "```python\n",
    "Q1 = penguin['body_mass_g'].quantile(0.25)\n",
    "Q3 = penguin['body_mass_g'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove outliers\n",
    "filtered_penguin = penguin[(penguin['body_mass_g'] >= lower_bound) & (penguin['body_mass_g'] <= upper_bound)]\n",
    "\n",
    "# Compare correlation before and after\n",
    "print(\"Correlation before removing outliers:\", penguin.corr(numeric_only=True))\n",
    "print(\"Correlation after removing outliers:\", filtered_penguin.corr(numeric_only=True))\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**  \n",
    "1. **`IQR = Q3 - Q1`**:  \n",
    "   - Computes **Interquartile Range (IQR)** to detect outliers.  \n",
    "\n",
    "2. **`lower_bound = Q1 - 1.5 * IQR`** & **`upper_bound = Q3 + 1.5 * IQR`**:  \n",
    "   - Defines **outlier threshold** using **1.5x IQR rule**.  \n",
    "\n",
    "3. **`penguin[(penguin['body_mass_g'] >= lower_bound) & (penguin['body_mass_g'] <= upper_bound)]`**:  \n",
    "   - Creates a **filtered dataset** without extreme values.  \n",
    "\n",
    "4. **`corr()` Before & After**:  \n",
    "   - Computes correlation **before and after** removing outliers.  \n",
    "\n",
    "### **Why is This Important?**  \n",
    "- If correlation **changes significantly**, outliers are **influencing results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9236c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 13: Partial Correlation - Controlling for Other Variables\n",
    "\n",
    "Sometimes, two variables **appear correlated** only because they both depend on a **third variable**.  \n",
    "To check the **true correlation** between two variables **while controlling for another**, we use **Partial Correlation**.\n",
    "\n",
    "### **Formula for Partial Correlation (r_xy.z):**\n",
    "\\[ r_{xy.z} = \\frac{r_{xy} - r_{xz} r_{yz}}{\\sqrt{(1 - r_{xz}^2)(1 - r_{yz}^2)}} \\]\n",
    "\n",
    "Where:\n",
    "- **r_xy** = Pearson correlation between `x` and `y`.\n",
    "- **r_xz** = Pearson correlation between `x` and `z`.\n",
    "- **r_yz** = Pearson correlation between `y` and `z`.\n",
    "\n",
    "```python\n",
    "from pingouin import partial_corr\n",
    "\n",
    "# Compute Partial Correlation between bill_length_mm and body_mass_g, controlling for flipper_length_mm\n",
    "partial_corr_result = partial_corr(data=penguin, x='bill_length_mm', y='body_mass_g', covar='flipper_length_mm')\n",
    "\n",
    "print(partial_corr_result)\n",
    "```\n",
    "\n",
    "### **Why is This Important?**\n",
    "- If two variables **share a common influence**, **Partial Correlation** helps reveal the **true relationship**.\n",
    "- Useful when dealing with **confounding variables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pingouin import partial_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e069795",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_corr(data=penguin, x='bill_length_mm', y='body_mass_g', covar='flipper_length_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a6f00",
   "metadata": {},
   "source": [
    "\n",
    "## Step 14: Feature Selection Based on Correlation\n",
    "\n",
    "Highly correlated features can cause **redundancy** in machine learning models.  \n",
    "We **remove one of the features** if correlation **exceeds a threshold**.\n",
    "\n",
    "```python\n",
    "corr_matrix = penguin.corr(numeric_only=True)\n",
    "high_corr_features = set()\n",
    "\n",
    "threshold = 0.8  # Define strong correlation threshold\n",
    "\n",
    "for col in corr_matrix.columns:\n",
    "    for row in corr_matrix.index:\n",
    "        if abs(corr_matrix.loc[row, col]) > threshold and row != col:\n",
    "            high_corr_features.add(row)\n",
    "\n",
    "print(\"Highly correlated features:\", high_corr_features)\n",
    "```\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Helps in **dimensionality reduction**.\n",
    "- Prevents **overfitting** by removing **redundant variables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f51bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = penguin.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54175985",
   "metadata": {},
   "source": [
    "\n",
    "## Step 15: Correlation with Categorical Variables\n",
    "\n",
    "Pearson correlation **only works with numerical data**.  \n",
    "For **categorical variables**, we use:\n",
    "\n",
    "1. **Point-Biserial Correlation** (Binary vs. Numeric)\n",
    "2. **Cramer’s V** (Categorical vs. Categorical)\n",
    "\n",
    "```python\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "\n",
    "# Point-Biserial Correlation: Checking correlation between species (binary) and body mass\n",
    "penguin['species_binary'] = (penguin['species'] == 'Adelie').astype(int)\n",
    "r, p = pointbiserialr(penguin['species_binary'], penguin['body_mass_g'])\n",
    "print(f\"Point-Biserial Correlation: {r}, p-value: {p}\")\n",
    "\n",
    "# Cramér’s V: Checking association between two categorical variables (species & island)\n",
    "contingency_table = pd.crosstab(penguin['species'], penguin['island'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "cramers_v = np.sqrt(chi2 / (penguin.shape[0] * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "print(f\"Cramér’s V: {cramers_v}, p-value: {p}\")\n",
    "```\n",
    "\n",
    "### **Why is This Important?**\n",
    "- **Point-Biserial** helps analyze categorical/numeric relationships.\n",
    "- **Cramer’s V** helps measure association strength between **two categorical variables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777696c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr, chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6c4fe",
   "metadata": {},
   "source": [
    "\n",
    "## Step 16: Cross-Correlation for Time-Series Data\n",
    "\n",
    "If working with **time-series data**, we check **cross-correlation** (correlation at different time lags).  \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate time-series data\n",
    "np.random.seed(42)\n",
    "time_series = pd.DataFrame({\n",
    "    'time': np.arange(1, 101),\n",
    "    'feature_1': np.random.randn(100).cumsum(),\n",
    "    'feature_2': np.random.randn(100).cumsum()\n",
    "})\n",
    "\n",
    "# Compute lagged correlation (Shift feature_2 by 1 step)\n",
    "corr_lag1 = time_series['feature_1'].corr(time_series['feature_2'].shift(1))\n",
    "\n",
    "print(f\"Lagged Correlation (1 time-step lag): {corr_lag1}\")\n",
    "```\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Useful for detecting **leading/lagging relationships** in stock prices, climate patterns, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series['feature_1'].corr(time_series['feature_2'].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222c16c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 17: Rank Transformation for Robust Correlation\n",
    "\n",
    "Pearson correlation assumes **normally distributed data**.  \n",
    "If this assumption fails, we **apply rank transformation** for **more robust correlation measures**.\n",
    "\n",
    "```python\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Rank transform numerical features\n",
    "penguin['body_mass_rank'] = rankdata(penguin['body_mass_g'])\n",
    "\n",
    "# Compute Pearson correlation on ranked data\n",
    "ranked_corr = penguin[['body_mass_rank', 'bill_length_mm']].corr()\n",
    "\n",
    "print(ranked_corr)\n",
    "```\n",
    "\n",
    "### **Why is This Important?**\n",
    "- **Ranked correlation** is **less sensitive to outliers**.\n",
    "- Works well for **non-normal data distributions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin['body_mass_rank'] = rankdata(penguin['body_mass_g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df95881",
   "metadata": {},
   "source": [
    "\n",
    "## Step 18: Non-Parametric Bootstrap for Correlation Confidence Intervals\n",
    "\n",
    "We estimate **confidence intervals for correlation** using **bootstrap resampling**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Bootstrap resampling for correlation estimation\n",
    "bootstrap_corr = []\n",
    "for _ in range(1000):\n",
    "    sample = penguin.sample(frac=1, replace=True)\n",
    "    bootstrap_corr.append(sample['bill_length_mm'].corr(sample['body_mass_g']))\n",
    "\n",
    "# Compute 95% confidence interval\n",
    "ci_lower, ci_upper = np.percentile(bootstrap_corr, [2.5, 97.5])\n",
    "\n",
    "print(f\"95% Confidence Interval for Correlation: ({ci_lower}, {ci_upper})\")\n",
    "```\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Provides **uncertainty estimates** for correlation values.\n",
    "- Useful when **sample sizes are small**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cc667",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_corr = [penguin.sample(frac=1, replace=True)['bill_length_mm'].corr(penguin.sample(frac=1, replace=True)['body_mass_g']) for _ in range(1000)]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
