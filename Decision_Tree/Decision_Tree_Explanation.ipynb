{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caf5d9e",
   "metadata": {},
   "source": [
    "# **Decision Tree - The Ultimate Guide (Beginner to Advanced)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4860b1",
   "metadata": {},
   "source": [
    "## **1. Introduction to Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d25de",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a supervised learning algorithm used for both **classification** and **regression** problems. It is structured like a flowchart, where each internal node represents a **decision based on a feature**, each branch represents an **outcome**, and each leaf node represents a **final decision or class**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dfb1ea",
   "metadata": {},
   "source": [
    "Decision Trees are widely used because they are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ecebf2",
   "metadata": {},
   "source": [
    "- **Easy to understand & visualize**: The decision-making process is transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc3c5e",
   "metadata": {},
   "source": [
    "- **Versatile**: Can handle both numerical and categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ee830",
   "metadata": {},
   "source": [
    "- **Non-parametric**: No assumptions about data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3fb2d",
   "metadata": {},
   "source": [
    "- **Capable of handling missing values** using surrogate splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e178980",
   "metadata": {},
   "source": [
    "## **2. Understanding the Structure of a Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59730c5e",
   "metadata": {},
   "source": [
    "- **Root Node**: The topmost node where the first split occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e376d",
   "metadata": {},
   "source": [
    "- **Decision Nodes**: Internal nodes where further splits happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d83ce1",
   "metadata": {},
   "source": [
    "- **Leaf Nodes**: The final output (class label or regression value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0c50c",
   "metadata": {},
   "source": [
    "- **Splitting**: Process of dividing data based on conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6cb9b",
   "metadata": {},
   "source": [
    "- **Pruning**: Removing unnecessary splits to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c1476",
   "metadata": {},
   "source": [
    "- **Max Depth**: The maximum number of levels the tree can have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567c49a",
   "metadata": {},
   "source": [
    "- **Impurity**: How mixed the data is at a node (lower impurity is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"![](https://github.com/rakeshravikumar-ML/ml_algorithm_helper/blob/01f99c916eb0b62ca90145da664e0ff9287cc59d/Decision_Tree/Decision_Tree.png)\\n\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69c57e",
   "metadata": {},
   "source": [
    "## **3. How a Decision Tree Works?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6842c2",
   "metadata": {},
   "source": [
    "A Decision Tree works by recursively partitioning the dataset at each node based on the best splitting criterion. The goal is to **reduce impurity** and make the groups as homogeneous as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc78de",
   "metadata": {},
   "source": [
    "### **Step-by-step working**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21c0bb",
   "metadata": {},
   "source": [
    "1. Select the **best feature** to split the data using a metric (Gini, Entropy, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa54e5",
   "metadata": {},
   "source": [
    "2. Recursively split the dataset into subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2bf512",
   "metadata": {},
   "source": [
    "3. Stop splitting when a stopping criterion is met (e.g., max depth, minimum samples per leaf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c29533",
   "metadata": {},
   "source": [
    "## **4. Impurity Measures: Entropy & Gini Impurity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d9c3e",
   "metadata": {},
   "source": [
    "### **4.1 Entropy (Used in ID3 Algorithm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd3b0e",
   "metadata": {},
   "source": [
    "Entropy measures the **randomness** or **uncertainty** in a dataset. The goal is to reduce entropy with each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc4a24",
   "metadata": {},
   "source": [
    "\\[ Entropy = - \\sum p_i \\log_2 p_i \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae6168",
   "metadata": {},
   "source": [
    "#### **Example Calculation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b338e5b",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with 10 observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520d0e8",
   "metadata": {},
   "source": [
    "- 6 belong to Class A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241c440",
   "metadata": {},
   "source": [
    "- 4 belong to Class B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44964ac8",
   "metadata": {},
   "source": [
    "Entropy = \\[ - (6/10 \\log_2 6/10 + 4/10 \\log_2 4/10) \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d6892",
   "metadata": {},
   "source": [
    "### **4.2 Gini Impurity (Used in CART Algorithm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a489c",
   "metadata": {},
   "source": [
    "Gini Impurity measures the probability of incorrect classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65cd7b",
   "metadata": {},
   "source": [
    "\\[ Gini = 1 - \\sum p_i^2 \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec63206",
   "metadata": {},
   "source": [
    "#### **Example Calculation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588288f2",
   "metadata": {},
   "source": [
    "For the same dataset (6 in Class A, 4 in Class B), Gini Impurity = \\[ 1 - (0.6^2 + 0.4^2) \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48274fa9",
   "metadata": {},
   "source": [
    "## **5. Information Gain & Splitting Criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55c4c1",
   "metadata": {},
   "source": [
    "Information Gain (IG) determines the best feature to split the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd8f0b",
   "metadata": {},
   "source": [
    "\\[ IG = \\text{Entropy before split} - \\text{Weighted Entropy after split} \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4958b8",
   "metadata": {},
   "source": [
    "The feature that **maximizes Information Gain** is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2f7ff",
   "metadata": {},
   "source": [
    "## **6. Overfitting & Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e9bf1",
   "metadata": {},
   "source": [
    "Overfitting occurs when a tree memorizes training data instead of learning patterns. Solutions include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b5211",
   "metadata": {},
   "source": [
    "- **Pre-pruning** (Limiting depth, minimum samples per leaf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455408c4",
   "metadata": {},
   "source": [
    "- **Post-pruning** (Trimming unnecessary branches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e83f1b",
   "metadata": {},
   "source": [
    "## **7. When & How to Use Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b93339",
   "metadata": {},
   "source": [
    "### **When to Use Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058ebcd",
   "metadata": {},
   "source": [
    "✅ When **interpretability** is crucial (e.g., medical diagnosis, finance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bcde2",
   "metadata": {},
   "source": [
    "✅ When working with **small to medium-sized datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc508e",
   "metadata": {},
   "source": [
    "✅ When handling **both numerical and categorical data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d099d",
   "metadata": {},
   "source": [
    "✅ When you need a model that doesn’t require feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9292a",
   "metadata": {},
   "source": [
    "### **When NOT to Use Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315b3ac",
   "metadata": {},
   "source": [
    "❌ When dataset is **too large** → Trees can overfit or be slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de7142",
   "metadata": {},
   "source": [
    "❌ When **complex relationships** exist → Neural Networks or SVMs might be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea483da",
   "metadata": {},
   "source": [
    "❌ When **very high accuracy is needed** → Use ensembles like **Random Forest**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863491",
   "metadata": {},
   "source": [
    "## **8. Advantages & Disadvantages of Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ad64c",
   "metadata": {},
   "source": [
    "### **Advantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8b8f4",
   "metadata": {},
   "source": [
    "✔ **Simple & interpretable** - Can be visualized as a flowchart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3c337",
   "metadata": {},
   "source": [
    "✔ **No need for feature scaling** - Works with raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67661b",
   "metadata": {},
   "source": [
    "✔ **Handles categorical & numerical features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594aa55b",
   "metadata": {},
   "source": [
    "✔ **Fast for small datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3962530",
   "metadata": {},
   "source": [
    "### **Disadvantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822d60b",
   "metadata": {},
   "source": [
    "❌ **Prone to overfitting** - If too deep, it memorizes data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71c2cb",
   "metadata": {},
   "source": [
    "❌ **Sensitive to small changes** - A slight change in data can alter structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849a33d",
   "metadata": {},
   "source": [
    "❌ **Not great for large datasets** - Slower than linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac11aa",
   "metadata": {},
   "source": [
    "## **9. Implementing Decision Tree in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb7d8e",
   "metadata": {},
   "source": [
    "Now, let's implement a Decision Tree Classifier using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import math\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(class_counts):\n",
    "    total = sum(class_counts)\n",
    "    return -sum((count / total) * math.log2(count / total) for count in class_counts if count != 0)\n",
    "\n",
    "# Example Calculation: 6 in Class A, 4 in Class B\n",
    "class_counts = [6, 4]\n",
    "entropy_value = entropy(class_counts)\n",
    "print(f'Entropy: {entropy_value:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634db567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gini_impurity(class_counts):\n",
    "    total = sum(class_counts)\n",
    "    return 1 - sum((count / total) ** 2 for count in class_counts)\n",
    "\n",
    "# Example Calculation: 6 in Class A, 4 in Class B\n",
    "gini_value = gini_impurity(class_counts)\n",
    "print(f'Gini Impurity: {gini_value:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {'Study_Hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'Pass_Exam': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]}  # 1 = Pass, 0 = Fail\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02259263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[['Study_Hours']]\n",
    "y = df['Pass_Exam']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecef902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "tree.plot_tree(clf, filled=True, feature_names=['Study_Hours'], class_names=['Fail', 'Pass'])\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
